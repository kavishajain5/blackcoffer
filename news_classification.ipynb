{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXqWV1ruvQhjbruL85ttNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavishajain5/Ineuron-Internship-/blob/main/news_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgD8x8Dps3-S"
      },
      "outputs": [],
      "source": [
        "%%writefile kaggle.json\n",
        "{\"username\":\"Kavisha23Jain\",\"key\":\"*********************************\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c learn-ai-bbc\n",
        "! unzip /content/learn-ai-bbc.zip"
      ],
      "metadata": {
        "id": "5K0ua23ntDoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, SpatialDropout1D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n"
      ],
      "metadata": {
        "id": "aNv5zdlXws7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Downloading necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords[0:10]\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "I8ZFFkXRwrXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/BBC News Train.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bd8MRLFPtMxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_FQTz6u-d5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Category.value_counts()"
      ],
      "metadata": {
        "id": "q5JDHt0GtRc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "7Duohw7jtVOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "EDPqHMqst7Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['business', 'politics', 'entertainment', 'sport', 'tech']\n",
        "df['Category'] = df['Category'].replace(categories, [0, 1, 2, 3, 4])"
      ],
      "metadata": {
        "id": "W0IroglCt5dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wc_stopwords = set(STOPWORDS)\n",
        "\n",
        "business_data = df[df['Category'] == 0].Text\n",
        "wordcloud = WordCloud(stopwords = wc_stopwords, max_words = 200, background_color = 'white', width = 1200, height = 800).generate(\" \".join(business_data))\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.title('WordCloud for Business News Articles\\n')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZMgHmmECt_gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "education_data = df[df['Category'] == 1].Text\n",
        "wordcloud = WordCloud(stopwords = wc_stopwords, max_words = 200, background_color = 'white', width = 1200, height = 800).generate(\" \".join(education_data))\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.title('WordCloud for Polical News Articles\\n')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H1q1IQ7fuBkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entertainment_data = df[df['Category'] == 2].Text\n",
        "wordcloud = WordCloud(stopwords = wc_stopwords, max_words = 200, background_color = 'white', width = 1200, height = 800).generate(\" \".join(entertainment_data))\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.title('WordCloud for Entertainment News Articles\\n')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WpScpuMRuKA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sports_data = df[df['Category'] == 3].Text\n",
        "wordcloud = WordCloud(stopwords = wc_stopwords, max_words = 200, background_color = 'white', width = 1200, height = 800).generate(\" \".join(sports_data))\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.title('WordCloud for Sports News Articles\\n')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oYhB58iQuBbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "technology_data = df[df['Category'] == 4].Text\n",
        "wordcloud = WordCloud(stopwords = wc_stopwords, max_words = 200, background_color = 'white', width = 1200, height = 800).generate(\" \".join(technology_data))\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.title('WordCloud for Technology News Articles\\n')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BB2ZDpqVuBRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text preprocessing"
      ],
      "metadata": {
        "id": "IIBe3vwzwio4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chars(text):\n",
        "    return re.sub('[^A-Za-z0-9 ]+', '', text)\n",
        "\n",
        "def decontractions(phrase):\n",
        "    \"\"\"Decontracted takes text and converts contractions into their natural form.\"\"\"\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def stopwords(text):\n",
        "    text = text.split(' ')\n",
        "    output = [i for i in text if i not in stopwords]\n",
        "    return ' '.join(output)\n",
        "def stemming(text):\n",
        "    text = text.split(' ')\n",
        "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return ' '.join(stem_text)\n",
        "def lemmatizer(text):\n",
        "    text = text.split(' ')\n",
        "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return ' '.join(lemm_text)\n",
        "\n",
        "def preprocessing(text_df):\n",
        "    text_df = text_df.apply(lambda x : chars(x))\n",
        "    text_df = text_df.apply(lambda x : x.lower())\n",
        "    text_df = text_df.apply(lambda x : decontractions(x))\n",
        "    text_df = text_df.apply(lambda x: remove_stopwords(x))\n",
        "    text_df = text_df.apply(lambda x: stemming(x))\n",
        "    text_df = text_df.apply(lambda x: lemmatizer(x))\n",
        "    return text_df\n"
      ],
      "metadata": {
        "id": "9enGJtZftksO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "NQydaTQt77YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_train = preprocessing(X_train)"
      ],
      "metadata": {
        "id": "4UEcM2aLzP6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tf = vectorizer.fit_transform(process_train.values)\n",
        "X_test_tf = vectorizer.transform(process_test.values)\n",
        "X_train_tf"
      ],
      "metadata": {
        "id": "WZf291hA8Tpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# save the model to disk\n",
        "filename = 'tfidf_vector.model'\n",
        "pickle.dump(vectorizer, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "GLH7ar9T8TkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "tsne_model = TSNE(n_components = 2, random_state = 0)\n",
        "tsne_data = tsne_model.fit_transform(X_train_tf)"
      ],
      "metadata": {
        "id": "tVsTz6Ob8TjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe from tsne\n",
        "tsne_data = np.vstack((tsne_data.T, y_train)).T\n",
        "tsne_df = pd.DataFrame(data = tsne_data,columns =(\"Dim_1\", \"Dim_2\", \"label\"))"
      ],
      "metadata": {
        "id": "oBHAH-3u8TfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(y_train)\n",
        "y_train_tr = le.transform(y_train)\n",
        "y_test_tr = le.transform(y_test)"
      ],
      "metadata": {
        "id": "HK22WaZp8TdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "uxoEjTyn8uCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf = DecisionTreeClassifier(random_state=0).fit(X_train_tf, y_train_tr)\n",
        "print('Training score of model is',dt_clf.score(X_train_tf, y_train_tr))\n",
        "y_pred = dt_clf.predict(X_test_tf)\n",
        "print(classification_report(y_test_tr, y_pred, target_names=le.classes_))\n",
        "sns.heatmap(confusion_matrix(y_test_tr, y_pred), annot=True, linewidths=.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e39fnWnM8t53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(random_state=0).fit(X_train_tf, y_train_tr)\n",
        "print('Training score of model is',clf.score(X_train_tf, y_train_tr))\n",
        "y_pred = clf.predict(X_test_tf)\n",
        "print(classification_report(y_test_tr, y_pred, target_names=le.classes_))\n",
        "sns.heatmap(confusion_matrix(y_test_tr, y_pred), annot=True, linewidths=.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QKgBbURj8tvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ref : https://www.geeksforgeeks.org/saving-a-machine-learning-model/\n",
        "import pickle\n",
        "# save the model to disk\n",
        "filename = 'logistic_regression.model'\n",
        "pickle.dump(clf, open(filename, 'wb'))\n",
        "\n",
        "# # load the model from disk\n",
        "# loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "5_S-2w-M8_3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_news = '''Wipro: The IT major reported a 21 per cent decline in its June quarter net profit as higher employee-related costs pushed up the firm's overall expenses. Consolidated net profit at Rs 2,563.6 crore in April-June was 20.6 per cent, lower than Rs 3,242.6 crore net profit in the same period a year back.\n",
        "NTPC: The state-owned power giant has inked a pact with Moroccan Agency for Sustainable Energy (MASEN) for cooperation in renewable energy. It signed an MoU with MASEN for cooperation in the renewable energy sector during the 17th CII EXIM Conclave on India Africa Growth Partnership held in New Delhi from 19th-20th July 2022.'''"
      ],
      "metadata": {
        "id": "HgRqbQZO9HoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_news = all_preprocessing(pd.Series(new_news))\n",
        "vec_news = vectorizer.transform(pre_news)\n",
        "news_pred = clf.predict(vec_news)\n",
        "print('New news classified as {0} category'.format(le.classes_[news_pred][0]))"
      ],
      "metadata": {
        "id": "FuJ-bZj19KEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# saving\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(token, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# # loading\n",
        "# with open('tokenizer.pickle', 'rb') as handle:\n",
        "#     token = pickle.load(handle)"
      ],
      "metadata": {
        "id": "D-ixw31r9SYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lenc = LabelEncoder()\n",
        "y_train_class = lenc.fit_transform(y_train)\n",
        "y_test_class = lenc.transform(y_test)\n",
        "\n",
        "y_train_class = to_categorical(y_train_class, num_classes=5)"
      ],
      "metadata": {
        "id": "sMPoJL-89eVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_input = len(token.word_index.keys()) + 1"
      ],
      "metadata": {
        "id": "wvuUrDws9jfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lz3Khcal9nnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = Input(shape=(max_len,))\n",
        "embed = Embedding(embed_input, 100)(input)\n",
        "lstm1 = LSTM(100, return_sequences=True, return_state=False)(embed)\n",
        "lstm2 = LSTM(64, dropout=0.2)(lstm1)\n",
        "dense1 = Dense(64)(lstm2)\n",
        "drop1 = Dropout(0.3)(dense1)\n",
        "dense2 = Dense(24)(drop1)\n",
        "dense3 = Dense(5, activation='softmax')(dense2)\n",
        "\n",
        "model = Model(inputs=input, outputs=dense3)"
      ],
      "metadata": {
        "id": "GqhsUpyA9nOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "XZgk4cU89pC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "callback = EarlyStopping(monitor='val_accuracy', verbose=1, patience=3)\n",
        "\n",
        "history = model.fit(data_xtrain_pad, y_train_class, validation_split=0.1, epochs=70, use_multiprocessing=True)"
      ],
      "metadata": {
        "id": "Z3CtAXGt9pBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(121)\n",
        "sns.lineplot(epochs, accuracy, label='Training accuracy')\n",
        "sns.lineplot(epochs, val_accuracy, label='Validation accuracy')\n",
        "plt.title('Training and validation accuracyuracy')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(122)\n",
        "sns.lineplot(epochs, loss, label='Training loss')\n",
        "sns.lineplot(epochs, val_loss, label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r8xij9LO9o-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = lenc.transform(y_test)\n",
        "y_pred = model.predict(data_xtest_pad)\n",
        "y_pred = np.argmax(y_pred,axis=1)"
      ],
      "metadata": {
        "id": "PUtT8L1q9o8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = lenc.classes_\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, linewidths=.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8YC4sXYk96hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We choose Logistic Regresion model to use while deployment"
      ],
      "metadata": {
        "id": "ThUtZSLZ-Kxv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FiRY5ji96eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjpa2rPM96cM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}